# The Hadoop Configuration to use when communicating with a Hadoop cluster. This is used for all Hadoop client tools
# including HDFS, Hive, HBase, and Sqoop.
# For more configuration options specific to the Hadoop configuration choosen
# here see the config.properties file in that configuration's directory.
# Note: should no longer be used and will be ignored after named cluster configurations have been updated for 9.0+
# Will only be referenced if site configuration files are not found in the expected locations in the metastore folders
active.hadoop.configuration=

# If using shim configurations from 8.3 and prior and have not migrated to 9.0 configurations:
#     Path to the directory that contains the available Hadoop configurations
# If using shim configurations from 9.0+:
#     Path to metastore to use when running on a Pentaho slave server (e.g. Pan or Kitchen)
#     To use an existing PDI metastore, for example, the directory is /home/user/.pentaho
hadoop.configurations.path=hadoop-configurations

# Version of Kettle to use from the Kettle HDFS installation directory. This can be set globally here or overridden per job
# as a User Defined property. If not set we will use the version of Kettle that is used to submit the Pentaho MapReduce job.
pmr.kettle.installation.id=

# Installation path in HDFS for the Pentaho MapReduce Hadoop Distribution
# The directory structure should follow this structure where {version} can be configured through the Pentaho MapReduce
# User Defined properties as kettle.runtime.version
#
# /opt/pentaho/mapreduce/
#  +- {version}/
#  |   +- lib/
#  |       ..
#  |       +- kettle-core-{version}.jar
#  |       +- kettle-engine-{version}.jar
#  |       ..
#  |   +- plugins/
#  |       pentaho-big-data-plugin/
#  |        .. (additional optional plugins)
#  +- {another-version}/
#  |   +- lib/
#  |       ..
#  |       +- kettle-core-{version}.jar
#  |       +- kettle-engine-{version}.jar
#  |       ..
#  |   +- plugins/
#  |       +- pentaho-big-data-plugin/
#  |           ..
#  |       +- my-custom-plugin/
#  |           ..
pmr.kettle.dfs.install.dir=/opt/pentaho/mapreduce

# Enables the use of Hadoop's Distributed Cache to store the Kettle environment required to execute Pentaho MapReduce
# If this is disabled you must configure all TaskTracker nodes with the Pentaho for Hadoop Distribution
# @deprecated This is deprecated and is provided as a migration path for existing installations.
pmr.use.distributed.cache=true

# Pentaho MapReduce runtime archive to be preloaded into kettle.hdfs.install.dir/pmr.kettle.installation.id
pmr.libraries.archive.file=pentaho-mapreduce-libraries.zip

# Additional plugins to be copied when Pentaho MapReduce's Kettle Environment does not exist on DFS. This should be a comma-separated
# list of plugin folder names to copy.
# e.g. pmr.kettle.additional.plugins=my-test-plugin,steps/DummyPlugin
pmr.kettle.additional.plugins=pdi-core-plugins,pentaho-metastore-locator-plugin

# Individual file name prefixes to not include when adding plugins to the Pentaho MapReduce Kettle Environment. This should be a comma-separated
# list of file prefixes to exclude.  The pdi-core-plugins-ui value should not be removed, only added to as demonstrated below.
# e.g. to exclude the file some-jar-file-name-9.0.0.0-xxx.jar: pmr.kettle.exclude.plugin.files=pdi-core-plugins-ui,some-jar-file-name
pmr.kettle.exclude.plugin.files=pdi-core-plugins-ui

notificationsBeforeLoadingShim=1
maxTimeoutBeforeLoadingShim=300

# pmr.create.unique.metastore.dir:
# If the property is not present or set to true, a unique metastore directory is created for each execution of a Pentaho MapReduce job
# If the property is set to false, a single metastore directory is created and is overwritten for each Pentaho MapReduce job
# Setting this property to false will save space within HDFS but can cause concurrency and security issues if multiple users are using the same 
# pmr.kettle.dfs.install.dir.
pmr.create.unique.metastore.dir=true

# Value to use for the ipc.client.connect.max.retries.on.timeouts Hadoop property when connecting to HDFS
# Note that this value overrides any ipc.client.connect.max.retries.on.timeouts set in *site.xml files for individual
# Hadoop cluster definitions
hadoopfs.ipc.client.connect.max.retries.on.timeouts=5

# These clauses are added on the java command line when executing a "Start PDI Cluster on Yarn" step
yarn.additional.jvm.options=--add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/java.lang.reflect=ALL-UNNAMED --add-opens java.base/sun.net.www.protocol.jar=ALL-UNNAMED --add-opens java.base/java.util=ALL-UNNAMED --add-opens java.base/java.io=ALL-UNNAMED --add-opens java.base/java.net=ALL-UNNAMED --add-opens java.base/java.security=ALL-UNNAMED --add-opens java.base/sun.net.www.protocol.file=ALL-UNNAMED --add-opens java.base/java.security=ALL-UNNAMED --add-opens java.base/sun.net.www.protocol.file=ALL-UNNAMED --add-opens java.base/sun.net.www.protocol.ftp=ALL-UNNAMED --add-opens java.base/sun.net.www.protocol.http=ALL-UNNAMED --add-opens java.base/sun.reflect.misc=ALL-UNNAMED --add-opens java.management/javax.management=ALL-UNNAMED --add-opens java.management/javax.management.openmbean=ALL-UNNAMED --add-opens java.naming/com.sun.jndi.ldap=ALL-UNNAMED --add-opens java.base/java.math=ALL-UNNAMED --add-opens java.base/java.io=ALL-UNNAMED --add-opens java.base/java.lang.Object=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED
