# The Hadoop Configuration to use when communicating with a Hadoop cluster. This is used for all Hadoop client tools
# including HDFS, Hive, HBase, and Sqoop.
# For more configuration options specific to the Hadoop configuration choosen
# here see the config.properties file in that configuration's directory.
active.hadoop.configuration=

# Path to the directory that contains the available Hadoop configurations
hadoop.configurations.path=hadoop-configurations

# Version of Kettle to use from the Kettle HDFS installation directory. This can be set globally here or overridden per job
# as a User Defined property. If not set we will use the version of Kettle that is used to submit the Pentaho MapReduce job.
pmr.kettle.installation.id=

# Installation path in HDFS for the Pentaho MapReduce Hadoop Distribution
# The directory structure should follow this structure where {version} can be configured through the Pentaho MapReduce
# User Defined properties as kettle.runtime.version
#
# /opt/pentaho/mapreduce/
#  +- {version}/
#  |   +- lib/
#  |       ..
#  |       +- kettle-core-{version}.jar
#  |       +- kettle-engine-{version}.jar
#  |       ..
#  |   +- plugins/
#  |       pentaho-big-data-plugin/
#  |        .. (additional optional plugins)
#  +- {another-version}/
#  |   +- lib/
#  |       ..
#  |       +- kettle-core-{version}.jar
#  |       +- kettle-engine-{version}.jar
#  |       ..
#  |   +- plugins/
#  |       +- pentaho-big-data-plugin/
#  |           ..
#  |       +- my-custom-plugin/
#  |           ..
pmr.kettle.dfs.install.dir=/opt/pentaho/mapreduce

# Enables the use of Hadoop's Distributed Cache to store the Kettle environment required to execute Pentaho MapReduce
# If this is disabled you must configure all TaskTracker nodes with the Pentaho for Hadoop Distribution
# @deprecated This is deprecated and is provided as a migration path for existing installations.
pmr.use.distributed.cache=true

# Pentaho MapReduce runtime archive to be preloaded into kettle.hdfs.install.dir/pmr.kettle.installation.id
pmr.libraries.archive.file=pentaho-mapreduce-libraries.zip

# Additional plugins to be copied when Pentaho MapReduce's Kettle Environment does not exist on DFS. This should be a comma-separated
# list of plugin folder names to copy.
# e.g. pmr.kettle.additional.plugins=my-test-plugin,steps/DummyPlugin
pmr.kettle.additional.plugins=pdi-core-plugins

notificationsBeforeLoadingShim=1
maxTimeoutBeforeLoadingShim=300

# pmr.create.unique.metastore.dir:
# If the property is not present or set to true, a unique metastore directory is created for each execution of a Pentaho MapReduce job
# If the property is set to false, a single metastore directory is created and is overwritten for each Pentaho MapReduce job
# Setting this property to false will save space within HDFS but can cause concurrency and security issues if multiple users are using the same 
# pmr.kettle.dfs.install.dir.
pmr.create.unique.metastore.dir=true
